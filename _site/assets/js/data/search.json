[
  
  {
    "title": "Rook ceph: cloud-native storage orchestrator + distributed storage system",
    "url": "/posts/rook-ceph/",
    "categories": "storage",
    "tags": "ceph",
    "date": "2025-10-17 07:39:00 +0530",
    "content": "Prerequisites:     Kubernetes v1.16 (or) higher   Helm 3.x   Raw devices (no partitions or formatted filesystems)   Raw partitions (no formatted filesystem)   LVM Logical Volumes (no formatted filesystem)   Persistent Volumes available from a storage class in block mode   Installation Requirements:     Ceph OSDs have a dependency on LVM when OSDs are created on raw devices (or) partitions.   Reset the disk used by rook for osds.   Prepare nodes for ceph osds:  Rest the disks on all hosts(worker nodes) to usable state  df -h  DISK=\"/dev/vdX\"  sgdisk --zap-all $DISK  lsblk -f   Install lvm2 package on all the hosts(worker nodes) where OSDs will be running.  apt-get update -y   apt-get install -y lvm2   Installing Ceph Operator with Helm: The Ceph Operator helm chart will install the basic components necessary to create a storage platform for your Kubernetes cluster.  Add the rook-ceph Helm repository: helm repo add rook-release https://charts.rook.io/release    Fetch the latest charts from the repository: helm repo update   Retrieve the package from rook-release repository, and download it locally:  helm fetch rook-release/rook-ceph --untar   Install ceph operator in the rook-ceph namespace:  helm install --create-namespace --namespace rook-ceph rook-ceph rook-release/rook-ceph -f values.yaml --version 1.18   To confirm that the deployment succeeded, run: kubectl -n rook-ceph get pod   Installing Ceph cluster with Helm:  helm fetch rook-release/rook-ceph-cluster --untar   helm install --namespace rook-ceph rook-ceph-cluster \\    --set operatorNamespace=rook-ceph rook-release/rook-ceph-cluster -f values.yaml --version 1.18   Deploy the rook toolbox to run arbitrary Ceph commands kubectl create -f https://raw.githubusercontent.com/rook/rook/release-1.18/deploy/examples/toolbox.yaml   Once the rook-ceph-tools pod is running, we can connect to it with: kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash   bash-4.4$ ceph osd status ID  HOST                    USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE  0  k8s-worker01-ceph  21.1M   199G      0        0       0        0   exists,up  1  k8s-worker03-ceph  21.1M   199G      0        0       0        0   exists,up  2  k8s-worker02-ceph  21.1M   199G      0        0       0        0   exists,up  bash-4.4$ ceph df --- RAW STORAGE --- CLASS     SIZE    AVAIL    USED  RAW USED  %RAW USED hdd    600 GiB  600 GiB  63 MiB    63 MiB       0.01 TOTAL  600 GiB  600 GiB  63 MiB    63 MiB       0.01  --- POOLS --- POOL         ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL .mgr          1    1  449 KiB        2  1.3 MiB      0    190 GiB replicapool   2   32     19 B        1   12 KiB      0    190 GiB   Accessing the ceph dashboard:  Get the external service IP: kubectl get svc -n rook-ceph  NAME                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S) rook-ceph-mgr             ClusterIP   X.X.X.X     &lt;none&gt;        9283/TCP             rook-ceph-mgr-dashboard   ClusterIP   X.X.X.X    &lt;none&gt;        7000/TCP   Use CLUSTER-IP of the rook-ceph-mgr-dashboard to access the dashboard using port forward:  kubectl port-forward  --address 0.0.0.0  service/rook-ceph-mgr-dashboard 8443:7000 -n rook-ceph   http://node-ip:8443   Login Credentials: default user named `admin`  To retrieve the generated password, we can run the following: kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=\"{['data']['password']}\" | base64 --decode &amp;&amp; echo   Exposes the dashboard on the Internet (using an reverse proxy)  Create certificate resource to generate certs for ceph-dashboard  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata:   name: dashboard-cert   namespace: rook-ceph spec:   secretName: rook-ceph-secret      issuerRef:     name: ca-issuer     kind: ClusterIssuer   dnsNames:     - rook-ceph.com EOF   Create a ingress resource  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: rook-ceph-mgr-dashboard   namespace: rook-ceph   annotations:     kubernetes.io/tls-acme: \"false\"     nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"     nginx.ingress.kubernetes.io/server-snippet: |       proxy_ssl_verify off; spec:   ingressClassName: \"nginx\"   tls:    - hosts:      - rook-ceph.com      secretName: rook-ceph-secret   rules:   - host: rook-ceph.com     http:       paths:       - path: /         pathType: Prefix         backend:           service:             name: rook-ceph-mgr-dashboard             port:               number: 8443 EOF   You can now browse to https://rook-ceph.com/ to log into the dashboard.  Reference Links:          Rook           Rook github page"
  },
  
  {
    "title": "Let's build a HA kubernetes cluster on baremetal servers",
    "url": "/posts/k8s-kubeadm-ha-cluster/",
    "categories": "kubernetes",
    "tags": "kubernetes",
    "date": "2025-10-10 19:45:00 +0530",
    "content": "HA Kubernetes cluster with containerd  Prerequisites:    2 Ubuntu 24.04 LoadBalancer node’s   3 Ubuntu 24.04 Kubernetes master node’s   3 Ubuntu 24.04 Kubernetes worker node’s   Generate TLS certificates:  Prepare CFSSL on a Single Master Node (Certificate Authority Host): SSH into one of the master-node and run:  {   mkdir ~/certs &amp;&amp; cd ~/certs   wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64   wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64   chmod +x cfssl_linux-amd64 cfssljson_linux-amd64   mv cfssl_linux-amd64 /usr/local/bin/cfssl   mv cfssljson_linux-amd64 /usr/local/bin/cfssljson }   Create CA Config and CSR for Root Certificate Authority:  { cat &gt; ca-config.json &lt;&lt;EOF {     \"signing\": {         \"default\": {             \"expiry\": \"8766h\"         },         \"profiles\": {             \"kubernetes\": {                 \"expiry\": \"8766h\",                 \"usages\": [\"signing\",\"key encipherment\",\"server auth\",\"client auth\"]             }         }     } } EOF  cat &gt; ca-csr.json &lt;&lt;EOF {   \"CN\": \"Kubernetes\",   \"key\": {     \"algo\": \"rsa\",     \"size\": 2048   },   \"names\": [     {       \"C\": \"IN\",       \"L\": \"BLR\",       \"O\": \"Kubernetes\",       \"OU\": \"Cluster\",       \"ST\": \"KA\"     }   ] } EOF }   Generate CA cert and key:  cfssl gencert -initca ca-csr.json | cfssljson -bare ca  Create Server Certificate Signing Request (CSR) Including Full SANs:  cat &gt; kubernetes-csr.json &lt;&lt;EOF {   \"CN\": \"kubernetes\",   \"hosts\": [     \"127.0.0.1\",     \"kubernetes\",     \"kubernetes.default\",     \"kubernetes.default.svc\",     \"kubernetes.default.svc.cluster.local\",     \"192.168.122.60\",     # Load balancer VIP     \"192.168.122.101\",    # master01 IP     \"192.168.122.102\",    # master02 IP     \"192.168.122.103\",    # master03 IP     \"192.168.122.51\",     # load01 IP     \"192.168.122.52\"      # load02 IP   ],   \"key\": {     \"algo\": \"rsa\",     \"size\": 2048   },   \"names\": [     {       \"C\": \"IN\",       \"L\": \"BLR\",       \"O\": \"Kubernetes\",       \"OU\": \"Cluster\",       \"ST\": \"KA\"     }   ] } EOF  Note: Remove comments while creating the csr.  Generate the Server Certificate Pair:  {   cfssl gencert \\   -ca=ca.pem \\   -ca-key=ca-key.pem \\   -config=ca-config.json \\   -profile=kubernetes kubernetes-csr.json | \\    cfssljson -bare kubernetes }   Generate Client/Admin Certificate for kubeadm and kubectl: cat &gt; admin-csr.json &lt;&lt;EOF {   \"CN\": \"admin\",   \"key\": {     \"algo\": \"rsa\",     \"size\": 2048   },   \"names\": [     {       \"C\": \"IN\",       \"L\": \"BLR\",       \"O\": \"system:masters\",       \"OU\": \"Cluster\",       \"ST\": \"KA\"     }   ] } EOF   Generate client cert: {   cfssl gencert \\   -ca=ca.pem \\   -ca-key=ca-key.pem \\   -config=ca-config.json \\   -profile=kubernetes \\   admin-csr.json | cfssljson -bare admin }  Copy admin.pem and admin-key.pem to your workstation/user where kubectl runs.  Distribute Certificates Securely: {   mkdir -p /etc/kubernetes/pki }   { declare -a NODES=(192.168.122.101 192.168.122.102 192.168.122.103)  for node in ${NODES[@]}; do   scp ca.pem kubernetes.pem kubernetes-key.pem root@$node:/etc/kubernetes/pki/ done }   Setup kubernetes master &amp; worker nodes:  Run on all master &amp; worker nodes:  Load Kerenel Modules: { cat &lt;&lt; EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF  sudo modprobe overlay sudo modprobe br_netfilter }  Add Kernel Settings:  { cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF  sysctl --system }   Install container runtime: containerd  { wget https://github.com/containerd/containerd/releases/download/v2.1.4/containerd-2.1.4-linux-amd64.tar.gz tar Cxzvf /usr/local containerd-2.1.4-linux-amd64.tar.gz rm containerd-2.1.4-linux-amd64.tar.gz mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml }  sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml   { mkdir -pv /usr/local/lib/systemd/system/ wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -O /usr/local/lib/systemd/system/containerd.service }  { systemctl daemon-reload systemctl enable --now containerd systemctl start containerd systemctl status containerd }   Disable swap permanently:  {   swapoff -a   sed -i '/ swap / s/^/#/' /etc/fstab }  Install runc: { wget https://github.com/opencontainers/runc/releases/download/v1.3.0/runc.amd64 install -m 755 runc.amd64 /usr/local/sbin/runc }   Installing CNI plugins:  { wget https://github.com/containernetworking/plugins/releases/download/v1.7.1/cni-plugins-linux-amd64-v1.7.1.tgz mkdir -p /opt/cni/bin tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.7.1.tgz }  Install cricctl: { VERSION=\"v1.34.0\" curl -L https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-${VERSION}-linux-amd64.tar.gz --output crictl-${VERSION}-linux-amd64.tar.gz sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/bin rm -f crictl-$VERSION-linux-amd64.tar.gz }   Set containerd as default runtime for crictl: { cat &lt;&lt; EOF | sudo tee /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 2 debug: false pull-image-on-create: false EOF }   Add Apt repository &amp; Install Kubernetes components: { apt-get install -y apt-transport-https ca-certificates curl gpg mkdir /etc/apt/keyrings curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg cat &lt;&lt; EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ / EOF }    { apt-get update apt-get install -y kubelet=1.34.0-1.1 kubeadm=1.34.0-1.1 kubectl=1.34.0-1.1 apt-mark hold kubelet kubeadm kubectl }    ETCD cluster creation[On all etcd nodes]:  Note: Copy ca.pem, kubernetes.pem, kubernetes-key.pem to other master nodes.  { declare -a NODES=(192.168.122.101 192.168.122.102 192.168.122.103)  for node in ${NODES[@]}; do   scp ca.pem kubernetes.pem kubernetes-key.pem root@$node:  done }   {   mkdir -pv /etc/etcd /data/etcd   mv ca.pem kubernetes.pem kubernetes-key.pem /etc/etcd/   ll /etc/etcd/ }   Download etcd &amp; etcdctl binaries from Github:  {   ETCD_VER=v3.6.4   wget -q --show-progress \"https://github.com/etcd-io/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz\"   tar zxf etcd-${ETCD_VER}-linux-amd64.tar.gz   mv etcd-${ETCD_VER}-linux-amd64/etcd* /usr/local/bin/   rm -rf etcd* }  Create systemd unit file for etcd service:  Set NODE_IP to the correct IP of the machine where you are running this {  NODE_IP=\"k8s-master-1\"  ETCD1_IP=\"k8s-master-1\" ETCD2_IP=\"k8s-master-2\" ETCD3_IP=\"k8s-master-3\"   cat &lt;&lt;EOF &gt;/etc/systemd/system/etcd.service [Unit] Description=etcd  [Service] Type=notify ExecStart=/usr/local/bin/etcd \\\\   --name ${NODE_IP} \\\\   --cert-file=/etc/etcd/kubernetes.pem \\\\   --key-file=/etc/etcd/kubernetes-key.pem \\\\   --peer-cert-file=/etc/etcd/kubernetes.pem \\\\   --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\   --trusted-ca-file=/etc/etcd/ca.pem \\\\   --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\   --peer-client-cert-auth \\\\   --client-cert-auth \\\\   --initial-advertise-peer-urls https://${NODE_IP}:2380 \\\\   --listen-peer-urls https://${NODE_IP}:2380 \\\\   --advertise-client-urls https://${NODE_IP}:2379 \\\\   --listen-client-urls https://${NODE_IP}:2379,https://127.0.0.1:2379 \\\\   --initial-cluster-token etcd-cluster-0 \\\\   --initial-cluster ${ETCD1_IP}=https://${ETCD1_IP}:2380,${ETCD2_IP}=https://${ETCD2_IP}:2380,${ETCD3_IP}=https://${ETCD3_IP}:2380 \\\\   --initial-cluster-state new \\\\   --data-dir=/data/etcd Restart=on-failure RestartSec=5  [Install] WantedBy=multi-user.target EOF  }   Enable and Start etcd service: {   systemctl daemon-reload   systemctl enable --now etcd   systemctl start etcd   systemctl status etcd }  Verify Etcd cluster status: { ETCDCTL_API=3 etcdctl member list \\   --endpoints=https://127.0.0.1:2379 \\   --cacert=/etc/etcd/kubernetes.pem \\   --cert=/etc/etcd/kubernetes.pem \\   --key=/etc/etcd/kubernetes-key.pem }  Example output: 25ef0cb30a2929f1, started, k8s-master-1, https://k8s-master-1:2380, https://k8s-master-1:2379, false 5818a496a39840ca, started, k8s-master-2, https://k8s-master-2:2380, https://k8s-master-2:2379, false c669cf505c64b0e8, started, k8s-master-3, https://k8s-master-3:2380, https://k8s-master-3:2379, false  Repeat the above spets on other master nodes by replacing the ip address with respect to node ip address.  Initializing Master Node 1:  Create configuration file : { HAPROXY_IP=\"haproxy-vip\"  MASTER01_IP=\"k8s-master-1\" MASTER02_IP=\"k8s-master-2\" MASTER03_IP=\"k8s-master-3\"  cat &lt;&lt;EOF &gt; config.yaml apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration kubernetesVersion: stable apiServer:   certSANs:   - ${HAPROXY_IP}   - ${MASTER01_IP}   - ${MASTER02_IP}   - ${MASTER03_IP}   - \"127.0.0.1\"   - \"kubernetes\"   - \"kubernetes.default\"   - \"kubernetes.default.svc\"   - \"kubernetes.default.svc.cluster.local\"   ExtraArgs:     apiserver-count: \"3\" controlPlaneEndpoint: \"${HAPROXY_IP}:6443\" etcd:   external:     endpoints:     - https://${MASTER01_IP}:2379     - https://${MASTER02_IP}:2379     - https://${MASTER03_IP}:2379     caFile: /etc/etcd/ca.pem     certFile: /etc/etcd/kubernetes.pem     keyFile: /etc/etcd/kubernetes-key.pem networking:   podSubnet: 10.244.0.0/16 apiServerExtraArgs:   apiserver-count: \"3\" EOF }  Initializing the master node:  kubeadm init --config=config.yaml --upload-certs   {       mkdir -p $HOME/.kube   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config   sudo chown $(id -u):$(id -g) $HOME/.kube/config }   Initializing Master Node 2 &amp; 3 :  Use the kubeadm join command printed after master01 init, with the –control-plane flag and the certificate key uploaded:  sudo kubeadm join 192.168.122.60:6443 --token &lt;token&gt; \\   --discovery-token-ca-cert-hash sha256:&lt;hash&gt; \\   --control-plane --certificate-key &lt;certificate-key&gt;   Adding Worker nodes:  Get the join command from master node and run it on worker nodes to join the cluster:  kubeadm token create --print-join-command  When you run kubeadm init you will get the join command as follows , you need to run this on all worker nodes.  kubeadm join 192.168.X.X:6443 --token 85iw5v.ymo1wqcs9mrqmnnf --discovery-token-ca-cert-hash sha256:4710a65a4f0a2be37c03249c83ca9df2377b4433c6564db4e61e9c07f5a213dd   Activate the flannel CNI plugin:  kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml   If you use custom podCIDR (not 10.244.0.0/16) you first need to download the above manifest and modify the network to match your one  Reference Links:          Containerd           etcd           runc           cni plugins           cilium"
  },
  
  {
    "title": "How to setup loadbalancing with haproxy & keepalived",
    "url": "/posts/haproxy-keepalived/",
    "categories": "loadbalancer",
    "tags": "haproxy",
    "date": "2025-10-10 18:49:00 +0530",
    "content": "Prerequisites:    2 Ubuntu 24.04 node’s   HAProxy Configurations:  SSH to the nodes which will function as the load balancer and execute the following commands to install HAProxy. apt update &amp;&amp; apt install -y haproxy   Edit haproxy.cfg to connect it to the master nodes, set the correct values for &lt;loadbalancer-vip&gt; and &lt;kube-masterX-ip&gt; and add an extra entry for each additional master:  vim /etc/haproxy/haproxy.cfg   global     log /dev/log local0     log /dev/log local1 notice     daemon     maxconn 2000     user haproxy     group haproxy  defaults     log     global     mode    tcp     option  tcplog     option  dontlognull     retries 3     timeout connect 10s     timeout client 1m     timeout server 1m  listen stats     bind *:8404     mode http     stats enable     stats uri /stats     stats refresh 10s     stats show-node     stats auth admin:adminpass  frontend kubernetes-api     bind *:6443     default_backend kubernetes-masters  backend kubernetes-masters     balance roundrobin     option tcp-check \tserver k8s-master-0 &lt;kube-masterX-ip&gt;:6443 check \tserver k8s-master-1 &lt;kube-masterY-ip&gt;:6443 check     server k8s-master-2 &lt;kube-masterZ-ip&gt;:6443 check  Verify haproxy configuration &amp; restart HAproxy:  haproxy -f /etc/haproxy/haproxy.cfg -c   {     systemctl daemon-reload     sudo systemctl enable haproxy      sudo systemctl start haproxy     sudo systemctl status haproxy }  Install and configure Keepalived:  On both the nodes[master &amp; backup], run the following commands:  apt update &amp;&amp; apt install -y keepalived &amp;&amp; apt install -y libipset13  Keepalived Configurations:  On Master/Primary node:  vim /etc/keepalived/keepalived.conf   # Define the script used to check if haproxy is still working vrrp_script chk_haproxy {     script \"/usr/bin/killall -0 haproxy\"     interval 2     weight 2 }  # Configuration for Virtual Interface vrrp_instance LB_VIP {     interface ens6     state MASTER        # set to BACKUP on the peer machine     priority 301        # set to  300 on the peer machine     virtual_router_id 51      authentication {         auth_type user         auth_pass UGFzcwo=  # Password for accessing vrrpd. Same on all devices     }     unicast_src_ip &lt;lb-master-ip&gt; # IP address of master-lb     unicast_peer {         &lt;lb-backup-ip&gt;   # IP address of the backup-lb    }      # The virtual ip address shared between the two loadbalancers     virtual_ipaddress {         &lt;lb-vip&gt;    # vip      }     # Use the Defined Script to Check whether to initiate a fail over     track_script {         chk_haproxy     } }  On Backup/Secondary node:  vim /etc/keepalived/keepalived.conf   # Define the script used to check if haproxy is still working vrrp_script chk_haproxy {     script \"/usr/bin/killall -0 haproxy\"     interval 2     weight 2 }  # Configuration for Virtual Interface vrrp_instance LB_VIP {     interface ens6     state BACKUP        # set to BACKUP on the peer machine     priority 300        # set to  301 on the peer machine     virtual_router_id 51      authentication {         auth_type user         auth_pass UGFzcwo=  # Password for accessing vrrpd. Same on all devices     }     unicast_src_ip &lt;lb-backup-ip&gt; #IP address of backup-lb     unicast_peer {         &lt;lb-master-ip&gt;  #IP address of the master-lb    }      # The virtual ip address shared between the two loadbalancers     virtual_ipaddress {         &lt;lb-vip&gt;  #vip     }     # Use the Defined Script to Check whether to initiate a fail over.     track_script {         chk_haproxy     } }   Enable and restart keepalived service: {     systemctl enable --now keepalived     systemctl start keepalived     systemctl status keepalived }   Reference Links:          Keepalived           Haproxy"
  },
  
  {
    "title": "Running Jenkins on Kubernetes",
    "url": "/posts/Jenkins-on-kuberntes/",
    "categories": "jenkins",
    "tags": "jenkins",
    "date": "2024-11-04 11:25:00 +0530",
    "content": "Jenkins is a self-contained, open source automation server which can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software.  Prerequisites:     Kubernetes cluster   Helm 3.x   Installing Jenkins with Helm: Helm v3.0+ must be installed on your workstation.  Add the jenkins Helm repository: helm repo add jenkins https://charts.jenkins.io   Fetch the latest charts from the repository: helm repo update   Retrieve the package from jenkins repository, and download it locally:  helm fetch jenkins/jenkins --untar   Lets update values.yaml file as follow,  # Change admin password controller:   admin:     username: \"admin\"     password: 'SecureP@ssw0rd!'  # Install Additional Plugins controller:   additionalPlugins: ['pipeline-graph-view', 'job-dsl', 'junit', 'coverage', 'dark-theme']  # Configure Dark Theme controller:   JCasC:     configScripts:       dark-theme: |         appearance:           themeManager:             disableUserThemes: true             theme: \"dark\"  # Configure welcome message controller:   JCasC:     configScripts:       welcome-message: |         jenkins:           systemMessage: 🚀 Welcome To Jenkins Prod instance 🚀    Install jenkins in the jenkins namespace:  helm install jenkins jenkins/jenkins --values /tmp/jenkins/values.yaml -n jenkins --create-namespace   To confirm that the deployment succeeded, run: kubectl -n jenkins get pods   Get the Jenkins URL to visit by running these commands in the same shell:  kubectl patch svc jenkins --namespace jenkins -p '{\"spec\": {\"type\": \"NodePort\"}}'  export NODE_PORT=$(kubectl get --namespace jenkins -o jsonpath=\"{.spec.ports[0].nodePort}\" services jenkins)  export NODE_IP=$(kubectl get nodes --namespace jenkins -o jsonpath=\"{.items[0].status.addresses[0].address}\")  echo http://$NODE_IP:$NODE_PORT   jenkinsUrl must be set correctly else stylesheet won’t load and the dark theme.  controller:   jenkinsUrl: http://$NODE_IP:$NODE_PORT   jenkinsAdminEmail: mkbn@mkbn.com   # don't set these via JCasC, set as helm chart values   Run helm upgrade  helm upgrade jenkins jenkins/jenkins --values /tmp/jenkins/values.yaml -n jenkins   Login with the username: admin and password: SecureP@ssw0rd!  Expose jenkins ui using ingress  Install nginx ingress controller helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update  helm install ingress-nginx ingress-nginx/ingress-nginx \\ --namespace ingress-nginx --create-namespace   Check the status of the Ingress Controller: kubectl get pods -n ingress-nginx kubectl get svc -n ingress-nginx  Note: Add the EXTERNAL-IP address of your ingress controller to your domain’s DNS records as an A record.  Installing Cert-Manager helm repo add jetstack https://charts.jetstack.io --force-update  helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager --create-namespace \\ --version v1.16.1 \\ --set installCRDs=true   Verify Cert-Manager is running kubectl get pods -n cert-manager   Create the ClusterIssuer resource cat &lt;&lt;EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata:   name: letsencrypt-prod spec:   acme:     server: https://acme-v02.api.letsencrypt.org/directory     email: mkbn@mkbn.in     privateKeySecretRef:       name: letsencrypt-prod     solvers:     - http01:         ingress:           class: nginx EOF   Create the ingress resource cat &lt;&lt;EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: jenkins-ingress   namespace: jenkins   annotations:     cert-manager.io/cluster-issuer: \"letsencrypt-prod\"     nginx.ingress.kubernetes.io/rewrite-target: / spec:   ingressClassName: nginx   tls:   - hosts:     - jenkins.mkbn.in     secretName: jenkins-tls   rules:   - host: jenkins.mkbn.in     http:       paths:       - path: /         pathType: Prefix         backend:           service:             name: jenkins             port:               number: 8080 EOF   Now access the jenkins ui with https://jenkins.mkbn.in  Reference Links:          jenkins           nginx ingress controller           Cert-Manager"
  },
  
  {
    "title": "Deploy a kubernetes cluster using ansible",
    "url": "/posts/k8s-cluster-using-ansible/",
    "categories": "kuberntes",
    "tags": "ansible",
    "date": "2024-06-29 22:36:00 +0530",
    "content": "Deploying a Kubernetes cluster can be a daunting task, but with the right tools and guidance, it becomes manageable and efficient. In this blog post, I walk you through the process of setting up a Kubernetes cluster with single master and two worker nodes using Ansible role.  This setup is perfect for development and testing environments, providing a solid foundation to explore Kubernetes’ powerful orchestration capabilities. I will cover everything from preparing your environment to executing the Ansible playbook, ensuring you have a running cluster ready for your applications by the end of this guide.  Prerequisites:  Before we begin, there are a few prerequisites we need to address. Here’s what you’ll need:     3 ubuntu 22.04 virtual machines   2 vCPUs &amp; 4 GiB memory per node   A host with Ansible installed   First, you’ll need to fork and clone the repo.While you’re at it, give it a ⭐ too!  git clone https://github.com/MBN02/Ansible.git   Next, create a ansible.cfg file :  #example ansible.cfg file  [defaults] inventory       = /Users/mohan.kumar.bn/inventory command_warnings=False host_key_checking = false forks = 20 serial = 20 callback_whitelist = timer, profile_tasks gathering = smart stdout_callback = yaml color = true  [ssh_connection] pipelining = True   Creata an inventory file with your vm’s ip address  [control_plane] master-node ansible_host=192.168.X.A #repalce the ip  [workers] worker-node1 ansible_host=192.168.X.B #repalce the ip worker-node2 ansible_host=192.168.X.C #repalce the ip  [all:vars] ansible_python_interpreter=/usr/bin/python3  [control_plane:vars] ansible_ssh_private_key_file= /Users/mohan.kumar.bn/.ssh/id_rsa ansible_user=root  [workers:vars] ansible_ssh_private_key_file= /Users/mohan.kumar.bn/.ssh/id_rsa ansible_user=root   Next, create a playbook called  setup_kubernetes.yml --- - name: Setup Kubernetes Cluster   hosts: all   become: true   roles:     - setup-kuberntes-cluster   The final step is to execute the ansible role to bootstrap the cluster.  #ansible-playbook -i inventory playbook.yaml ansible-playbook setup_kubernetes.yml   Once done, login to controlplane node and run the following command to confirm if the cluster is created successfully.  kubectl get nodes   Reference Links:          Ansible           kubeadm"
  },
  
  {
    "title": "Install Rancher with Lets Encrypt on Kubernetes",
    "url": "/posts/rancher-installation/",
    "categories": "rancher",
    "tags": "rancher",
    "date": "2024-06-29 08:13:00 +0530",
    "content": "Rancher is a Kubernetes management tool to deploy and run clusters anywhere and on any provider.  Prerequisites:     Kubernetes cluster   Helm 3.x   Domain name and ability to perform DNS changes   Port 80 &amp; 443 must be accessible for Let’s Encrypt to verify and issue certificates   Pick a subdomain and create a DNS entry pointing to the IP Address that will be assigned to the Rancher Server.  Run the following command to find the IP Address. curl ifconfig.me   Create an A record with the IP Address in your DNS Provider. nslookup subdomain_name   Install cert-manager with Helm  Add the Helm repository:  helm repo add jetstack https://charts.jetstack.io --force-update   Update the helm chart repository: helm repo update   Install cert-manager:  helm install \\   cert-manager jetstack/cert-manager \\   --namespace cert-manager \\   --create-namespace \\   --version v1.15.3 \\   --set crds.enabled=true   Install Rancher:  Create cattle-system namesapce kubectl create ns cattle-system   Add the Helm repository  helm repo add rancher-latest https://releases.rancher.com/server-charts/latest   Update the helm chart repository: helm repo update   Deploy Rancher:  helm install rancher rancher-latest/rancher --namespace cattle-system \\    --set hostname=your_hostname \\    --set bootstrapPassword=Password \\    --set ingress.tls.source=letsEncrypt \\    --set letsEncrypt.email=email@address \\    --set letsEncrypt.ingress.class=nginx   kubectl -n cattle-system rollout status deploy/rancher  kubectl get pods -n cattle-system -w   Access Rancher User Interface https://rancher.url  Reference Links:          Installing Helm           Cert-manager           Rancher"
  },
  
  {
    "title": "PureLB & OpenELB: LB implementations for bare-metal K8s clusters",
    "url": "/posts/purellb-openlb-installation/",
    "categories": "kubernetes",
    "tags": "loadbalancer",
    "date": "2023-08-30 19:30:00 +0530",
    "content": "PureLB PureLB is a Service Load Balancer Controller for Kubernetes.  Instal PureLB using helm charts:  helm repo add purelb https://gitlab.com/api/v4/projects/20400619/packages/helm/stable  helm repo update  helm fetch purelb/purelb --untar  helm install --create-namespace --namespace=purelb purelb purelb/purelb -f values.yaml   Create IPv4 Service Group:  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: purelb.io/v1 kind: ServiceGroup metadata:   name: ipv4-routed   namespace: purelb spec:   local:     aggregation: /32     pool: 192.168.121.85-192.168.121.95     subnet: 192.168.121.1/24 EOF   Calico IPv4 Pool:  Layer 2 mode is the simplest to configure: in many cases, you don’t need any protocol-specific configuration, only IP addresses.  cat &lt;&lt;EOF | kubectl create -f - apiVersion: crd.projectcalico.org/v1 kind: IPPool metadata:   name: purelb-ipv4 spec:   cidr: 192.168.121.1/24   disabled: true EOF   Check if all the resources are deployed in purelb namespace:  kubectl get all -n purelb   PureLB Annotations: PureLB uses annotations to configure functionality not native in the k8s API.  apiVersion: v1 items: - apiVersion: v1   kind: Service   metadata:     annotations:       purelb.io/service-group: ipv4-routed #service group name   OpenELB OpenELB is an open-source load balancer implementation designed for bare-metal Kubernetes clusters.  Instal OpenELB using helm charts:  helm repo add kubesphere-stable https://charts.kubesphere.io/stable  helm repo update   helm fetch kubesphere-stable/openelb --untar  helm install openelb kubesphere-stable/openelb -n openelb-system --create-namespace -f values.yaml  Configure IP Address Pools Using Eip:  cat &lt;&lt;EOF | kubectl create -f - apiVersion: network.kubesphere.io/v1alpha2 kind: Eip metadata:     name: eip-dev-pool     annotations:       eip.openelb.kubesphere.io/is-default-eip: \"true\" spec:     address: 192.168.121.85-192.168.121.95     protocol: layer2     interface: eth0     disable: false status:     occupied: false     usage: 1     poolSize: 10     used:        \"192.168.121.91\": \"default/test-svc\"     firstIP: 192.168.121.85     lastIP: 192.168.121.95     ready: true     v4: true EOF   Test if OpenELB is assiging the ips:  kubectl run webapp --image=nginx kubectl expose pod webapp --port=80 --type=LoadBalancer  Reference Links:          PureLB Official Website           Gitlab page: PureLB           OpenELB"
  },
  
  {
    "title": "Meet Longhorn a cloud native distributed block storage for Kubernetes",
    "url": "/posts/longhorn-deploy/",
    "categories": "kubernetes",
    "tags": "longhorn",
    "date": "2023-08-04 12:56:00 +0530",
    "content": "Prerequisites:  Minimum Hardware requirements:     3 nodes   4 vCPUs per node   4 GiB per node   SSD/NVMe or similar performance block device on the node for storage   Installation Requirements:     A container runtime compatible with Kubernetes (Docker v1.13+, containerd v1.3.7+, etc.)   Kubernetes &gt;= v1.21   open-iscsi is installed, and the iscsid daemon is running on all the nodes.   RWX support requires that each node has a NFSv4 client installed.   The host filesystem supports the file extents feature to store the data. Currently longhorn support:            ext4       XFS           bash, curl, findmnt, grep, awk, blkid, lsblk must be installed.   Mount propagation must be enabled.   Install dependencies:  Install nfs-common, open-iscsi &amp; ensure daemon is running on all the nodes.  { sudo apt update sudo apt install -y nfs-common open-iscsi sudo systemctl enable open-iscsi --now systemctl status iscsid }   Run the Environment Check Script:  Note: jq[sudo apt install -y jq] maybe required to be installed locally prior to running env check script. curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.5.1/scripts/environment_check.sh | bash   Installing Longhorn with Helm: Helm v3.0+ must be installed on your workstation.  Add the Longhorn Helm repository: helm repo add longhorn https://charts.longhorn.io   Fetch the latest charts from the repository: helm repo update   Retrieve the package from longhorn repository, and download it locally:  helm fetch longhorn/longhorn --untar   Install Longhorn in the longhorn namespace:  helm install longhorn longhorn/longhorn --values /tmp/longhorn/values.yaml -n longhorn --create-namespace --version 1.7.2   To confirm that the deployment succeeded, run: kubectl -n longhorn get pod   Accessing the Longhorn UI:  Get the Longhorn’s external service IP: kubectl -n longhorn get svc  Use CLUSTER-IP of the longhorn-frontend to access the Longhorn UI using port forward:  kubectl port-forward svc/longhorn-frontend 8080:80 -n longhorn   Enabling basic authentication with ingress for longhorn UI Authentication is not enabled by default for kubectl and Helm installations.  Create a basic authentication file auth. It’s important the file generated is named auth (actually - that the secret has a key data.auth), otherwise the Ingress returns a 503.  USER=&lt;USERNAME_HERE&gt;; PASSWORD=&lt;PASSWORD_HERE&gt;; echo \"${USER}:$(openssl passwd -stdin -apr1 &lt;&lt;&lt; ${PASSWORD})\" &gt;&gt; auth  Create a secret:  kubectl -n longhorn create secret generic basic-auth --from-file=auth   Create the ingress resource:  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: longhorn-ingress   namespace: longhorn   annotations:     nginx.ingress.kubernetes.io/auth-type: basic     nginx.ingress.kubernetes.io/ssl-redirect: 'false'     nginx.ingress.kubernetes.io/auth-secret: basic-auth     nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required '     nginx.ingress.kubernetes.io/proxy-body-size: 10000m spec:   rules:   - http:       paths:       - pathType: Prefix         path: \"/\"         backend:           service:             name: longhorn-frontend             port:               number: 80 EOF   Reference Links:          longhorn           Ingress"
  },
  
  {
    "title": "Deploy kubernetes dashboard with user account + Ingress",
    "url": "/posts/kubernetes-dashboard/",
    "categories": "kubernetes",
    "tags": "dashboard",
    "date": "2023-08-03 09:56:00 +0530",
    "content": "Kubernetes dashboard is a web-based user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources.  Deploying the Dashboard UI  kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml   Create a service account  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata:   name: dashboard-admin   namespace: kubernetes-dashboard EOF   Create a ClusterRoleBinding  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:   name: admin-user roleRef:   apiGroup: rbac.authorization.k8s.io   kind: ClusterRole   name: cluster-admin subjects: - kind: ServiceAccount   name: dashboard-admin   namespace: kubernetes-dashboard EOF   Creating Bearer Token for ServiceAccount  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata:   name: dashboard-token   namespace: kubernetes-dashboard   annotations:     kubernetes.io/service-account.name: \"dashboard-admin\"    type: kubernetes.io/service-account-token  EOF   Create certificate resource to generate certs for kubernetes-dashboard  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata:   name: dashboard-cert   namespace: kubernetes-dashboard spec:   secretName: k8s-dashboard-secret      issuerRef:     name: ca-issuer     kind: ClusterIssuer   dnsNames:     - k8s.dashboard.com EOF   Create a ingress resource cat &lt;&lt;EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: dashboard-ingress   namespace: kubernetes-dashboard   annotations:     kubernetes.io/ingress.class: nginx     cert-manager.io/cluster-issuer: ca-issuer     nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" spec:   ingressClassName: nginx   rules:   - host: \"k8s.dashboard.com\"     http:       paths:       - path: /         pathType: Prefix         backend:           service:             name: kubernetes-dashboard             port:               number: 443   tls:   - hosts:     - k8s.dashboard.com     secretName: k8s-dashboard-secret EOF  Accessing the Dashboard UI  Go to “https://k8s.dashboard.com/” to access the dashboard.  Execute the following command to get the token which saved in the Secret:  kubectl get secret dashboard-token -n kubernetes-dashboard -o jsonpath={\".data.token\"} | base64 -d   Reference Links:          Kubernetes Dashboard           Ingress"
  },
  
  {
    "title": "How to install and configure Cert manager on kubernetes",
    "url": "/posts/cert-manager-installation/",
    "categories": "kubernetes",
    "tags": "cert-manager",
    "date": "2023-07-14 08:55:00 +0530",
    "content": "Install cert-manager with Helm  Add the Helm repository helm repo add jetstack https://charts.jetstack.io --force-update   Update the helm chart repository helm repo update   Install cert-manager helm install \\   cert-manager jetstack/cert-manager \\   --namespace cert-manager \\   --create-namespace \\   --version v1.19.1 \\   --set crds.enabled=true   Install cert-manager using kubectl apply  kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.19.1/cert-manager.yaml  ACME:  Configure ClusterIssuer for ACME (Let’s Encrypt Production) cat &lt;&lt; EOF | kubectl apply -f -  apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata:   name: letsencrypt-prod spec:   acme:     server: https://acme-v02.api.letsencrypt.org/directory     email: pes.mohan@gmail.com     privateKeySecretRef:       name: letsencrypt-prod     solvers:     - http01:         ingress:           class: nginx EOF   Status check: kubectl get certificate,clusterissuer,certificateRequest,order -A   SelfSigned:  Create a Certificate Authority  Create a CA private key openssl genrsa -out ca.key 2048  Create a CA certificate openssl req -new -x509 -sha256 -days 365 -key ca.key -out ca.crt  Import the CA certificate in the trusted Root Ca store of your clients  Create cluster issuer object  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata:   name: ca-issuer spec:   ca:     secretName: dev-ca-key-pair EOF   Create a secret that will be used for signing in cert-manager name space  Convert the content of the key and crt to base64  cat ca.crt | base64 -w 0 cat ca.key | base64 -w 0   apiVersion: v1 kind: Secret metadata:   name: dev-ca-key-pair   namespace: cert-manager data:   tls.crt:      $(base64-encoded cert data from tls-ingress.crt)   tls.key:      $(base64-encoded cert data from tls-ingress.key)   Create certificate resource to generate certs for your applications  A Certificate resource specifies fields that are used to generate certificate signing requests which are then fulfilled by the issuer type you have referenced.  This should be created in the same namespace where your application is installed  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata:   name: nginx-cert   namespace: nginx spec:   secretName: nginx-tls-secret      issuerRef:     name: ca-issuer     kind: ClusterIssuer   dnsNames:     - nginx.mkbn.tech EOF   This will create the certificate object in nginx ns , along with secret call nginx-tls-secret which can be used in our nginx-ingress config  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: nginx-ingress   namespace: nginx   annotations:     kubernetes.io/ingress.class: nginx     cert-manager.io/cluster-issuer: ca-issuer spec:   ingressClassName: nginx   rules:   - host: \"nginx.mkbn.tech\"     http:       paths:       - path: /         pathType: Prefix         backend:           service:             name: nginx             port:               number: 80   tls:   - hosts:     - nginx.mkbn-tech     secretName: nginx-tls-secret EOF   Reference Links:          Installing Helm           Cert-manager"
  },
  
  {
    "title": "How to install and configure MetalLB",
    "url": "/posts/Metallb-installation/",
    "categories": "kubernetes",
    "tags": "loadbalancer",
    "date": "2023-07-13 23:01:00 +0530",
    "content": "MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols  Instal MetalLB using kubernetes manifest:  kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.8/config/manifests/metallb-native.yaml   Configure MetatlLB IP range:  In order to assign an IP to the services, MetalLB must be instructed to do so via the IPAddressPool CR.  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata:   name: first-pool   namespace: metallb-system spec:   addresses:   - 192.168.229.80-192.168.229.90 EOF   Announce The Service IPs:  Layer 2 mode is the simplest to configure: in many cases, you don’t need any protocol-specific configuration, only IP addresses.  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata:   name: example   namespace: metallb-system spec:   ipAddressPools:   - first-pool EOF   Check if all the resources are deployed in metallb-system namespace:  kubectl get all -n metallb-system   Test if LB is assiging the ips:  kubectl run webapp --image=nginx kubectl expose pod webapp --port=80 --type=LoadBalancer  Reference Links:          MetalLb Official Website           Configure MetalLB"
  }
  
]

