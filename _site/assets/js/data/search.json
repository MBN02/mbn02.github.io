[
  
  {
    "title": "Install Rancher with Lets Encrypt on Kubernetes",
    "url": "/posts/rancher-installation/",
    "categories": "rancher",
    "tags": "rancher",
    "date": "2025-11-08 08:42:00 +0530",
    "content": "Rancher is a Kubernetes management tool to deploy and run clusters anywhere and on any provider.    üéûÔ∏è Watch Video  Prerequisites:     Kubernetes cluster   Helm 3.x   Domain name and ability to perform DNS changes   Install nginx ingress controller kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.13.3/deploy/static/provider/baremetal/deploy.yaml   Change the ‚Äòingress-nginx-controller‚Äô service type to LoadBalancer kubectl edit svc ingress-nginx-controller -n ingress-nginx   Create an A record with the IP Address of ‚Äòingress-nginx-controller‚Äô service in your domain ragistrar. nslookup subdomain_name   Install cert-manager with Helm  Add the Helm repository:  helm repo add jetstack https://charts.jetstack.io --force-update   Update the helm chart repository: helm repo update   Install cert-manager:  helm install \\   cert-manager jetstack/cert-manager \\   --namespace cert-manager \\   --create-namespace \\   --version v1.19.1 \\   --set crds.enabled=true  Generate a private CA and use it with Rancher via cert-manager  Create a config file ca.cnf with CA extensions:  [ req ] default_bits       = 2048 distinguished_name = req_distinguished_name x509_extensions    = v3_ca prompt             = no  [ req_distinguished_name ] CN = rancher-private-ca  [ v3_ca ] basicConstraints = CA:TRUE keyUsage = critical, digitalSignature, cRLSign, keyCertSign subjectKeyIdentifier=hash authorityKeyIdentifier=keyid:always,issuer:always   Generate CA key and cert openssl req -x509 -newkey rsa:2048 -sha256 -days 3650 \\   -keyout ca.key -out ca.crt -config ca.cnf -nodes   Generate Rancher Server TLS Cert Signed by Private CA  Create a CSR config rancher-csr.cnf [req] default_bits       = 2048 prompt             = no default_md         = sha256 distinguished_name = dn req_extensions     = req_ext  [dn] CN = rancher.mkbn.in  [req_ext] subjectAltName = @alt_names  [alt_names] DNS.1 = rancher.mkbn.in   Generate private key and CSR openssl genrsa -out tls.key 2048  openssl req -new -key tls.key -out tls.csr -config rancher-csr.cnf   Sign CSR with your CA openssl x509 -req -in tls.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\   -out tls.crt -days 365 -sha256 -extfile rancher-csr.cnf -extensions req_ext    Create cattle-system namesapce  kubectl create ns cattle-system   Create Kubernetes Secrets  Create CA certificate secret (generic).This secret is for Rancher to trust your private CA.  kubectl -n cattle-system create secret generic tls-ca --from-file=cacerts.pem=ca.crt kubectl -n cattle-system create secret generic tls-ca-additional --from-file=cacerts.pem=ca.crt   Create TLS secret for Rancher ingress.This secret holds your Rancher TLS cert and key signed by your private CA.  kubectl -n cattle-system create secret tls tls-rancher-ingress --cert=tls.crt --key=tls.key   Create cert-manager ClusterIssuer Using CA cat &lt;&lt;EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata:   name: rancher-ca-issuer spec:   ca:     secretName: tls-ca EOF  Install Rancher:  Add the rancher stable helm repository helm repo add rancher-stable https://releases.rancher.com/server-charts/stable   Add the Helm repository  helm repo update   Retrieve the package from rancher repository, and download it locally helm fetch rancher-stable/rancher --untar   Deploy Rancher helm upgrade --install rancher rancher-stable/rancher --namespace cattle-system \\   --set hostname=rancher.mkbn.in \\   --set bootstrapPassword=P@ssw0rd \\   --set ingress.tls.source=secret \\   --set ingress.tls.secretName=tls-rancher-ingress \\   --set ingress.ingressClassName=nginx \\   --set privateCA=true \\   --set additionalTrustedCAs=true \\   --set-string \"additionalTrustedCASecrets[0]=tls-ca\" \\   --set replicas=3   Verify that the Rancher Server is Successfully Deployed  kubectl get pods -n cattle-system -w   kubectl -n cattle-system rollout status deploy/rancher   Access Rancher User Interface https://rancher.url  üîó Reference Links:          Installing Helm           Cert-manager           Rancher"
  },
  
  {
    "title": "Meet Longhorn a cloud native distributed block storage for Kubernetes",
    "url": "/posts/longhorn-deploy/",
    "categories": "kubernetes",
    "tags": "longhorn",
    "date": "2025-11-06 08:42:00 +0530",
    "content": "Cloud native distributed block storage for Kubernetes    üéûÔ∏è Watch Video  Prerequisites:  Minimum Hardware requirements:     3 nodes   4 vCPUs per node   4 GiB per node   SSD/NVMe or similar performance block device on the node for storage   Installation Requirements:     A container runtime compatible with Kubernetes (Docker v1.13+, containerd v1.3.7+, etc.)   Kubernetes &gt;= v1.21   open-iscsi is installed, and the iscsid daemon is running on all the nodes.   RWX support requires that each node has a NFSv4 client installed.   The host filesystem supports the file extents feature to store the data. Currently longhorn support:            ext4       XFS           bash, curl, findmnt, grep, awk, blkid, lsblk must be installed.   Mount propagation must be enabled.   Install dependencies:  Install nfs-common, open-iscsi &amp; ensure daemon is running on all the nodes.  { sudo apt update sudo apt install -y nfs-common open-iscsi sudo systemctl enable open-iscsi --now systemctl status iscsid }   Run the Environment Check Script:  # For AMD64 platform curl -sSfL -o longhornctl https://github.com/longhorn/cli/releases/download/v1.10.0/longhornctl-linux-amd64  # For ARM platform curl -sSfL -o longhornctl https://github.com/longhorn/cli/releases/download/v1.10.0/longhornctl-linux-arm64  chmod +x longhornctl ./longhornctl check preflight --kubeconfig=.kube/config    Installing Longhorn with Helm: Helm v3.0+ must be installed on your workstation.  Add the Longhorn Helm repository: helm repo add longhorn https://charts.longhorn.io   Fetch the latest charts from the repository: helm repo update   Retrieve the package from longhorn repository, and download it locally:  helm fetch longhorn/longhorn --untar   Install Longhorn in the longhorn namespace:  helm install longhorn longhorn/longhorn --values longhorn/values.yaml -n longhorn-system --create-namespace --version 1.10.0   To confirm that the deployment succeeded, run: kubectl -n longhorn-system get pod   Enabling basic authentication with ingress for longhorn UI Authentication is not enabled by default for kubectl and Helm installations.  Note : Create a basic authentication file auth. It‚Äôs important the file generated is named auth (actually - that the secret has a key data.auth), otherwise the Ingress returns a 503.  USER=&lt;USERNAME_HERE&gt;; PASSWORD=&lt;PASSWORD_HERE&gt;; echo \"${USER}:$(openssl passwd -stdin -apr1 &lt;&lt;&lt; ${PASSWORD})\" &gt;&gt; auth  Create a secret:  kubectl -n longhorn-system create secret generic basic-auth --from-file=auth   Create the ingress resource:  cat &lt;&lt;EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata:   name: longhorn-frontend   namespace: longhorn-system   annotations:     nginx.ingress.kubernetes.io/auth-type: basic     nginx.ingress.kubernetes.io/auth-secret: basic-auth     nginx.ingress.kubernetes.io/auth-realm: \"Authentication Required\"     # cert-manager.io/cluster-issuer: \"letsencrypt-prod\"     # nginx.ingress.kubernetes.io/rewrite-target: / spec:   ingressClassName: nginx   tls:     - hosts:         - longhorn.mkbn.in       secretName: tls-longhorn-frontend   rules:     - host: longhorn.mkbn.in       http:         paths:         - path: /           pathType: Prefix           backend:             service:               name: longhorn-frontend               port:                 number: 80 EOF  Accessing the Longhorn UI:  https://longhorn.mkbn.in   Accessing without ingress:  Get the Longhorn‚Äôs external service IP:  kubectl -n longhorn-system get svc  Use CLUSTER-IP of the longhorn-frontend to access the Longhorn UI using port forward:  kubectl port-forward svc/longhorn-frontend 8080:80 -n longhorn-system   Create a demo StatefulSet using the default storage class:  Check out the github repo for code sample.  kubectl apply -f gitea-demo/gitea.yaml   üîó Reference Links:          longhorn           Ingress"
  },
  
  {
    "title": "How to setup loadbalancing with haproxy & keepalived",
    "url": "/posts/haproxy-keepalived/",
    "categories": "loadbalancer",
    "tags": "haproxy",
    "date": "2025-11-03 14:25:00 +0530",
    "content": "Haproxy with Keepalived    üéûÔ∏è Watch Video  Prerequisites:    2 Ubuntu 24.04 node‚Äôs   HAProxy Configurations:  SSH to the nodes which will function as the load balancer and execute the following commands to install HAProxy. apt update &amp;&amp; apt install -y haproxy   Edit haproxy.cfg to connect it to the master nodes, set the correct values for &lt;loadbalancer-vip&gt; and &lt;kube-masterX-ip&gt; and add an extra entry for each additional master:  vim /etc/haproxy/haproxy.cfg   global     log /dev/log local0     log /dev/log local1 notice     daemon     maxconn 2000     user haproxy     group haproxy  defaults     log     global     mode    tcp     option  tcplog     option  dontlognull     retries 3     timeout connect 10s     timeout client 1m     timeout server 1m  listen stats     bind *:8404     mode http     stats enable     stats uri /stats     stats refresh 10s     stats show-node     stats auth admin:adminpass  frontend kubernetes-api     bind *:6443     default_backend kubernetes-masters  backend kubernetes-masters     balance roundrobin     option tcp-check \tserver k8s-master-0 &lt;kube-masterX-ip&gt;:6443 check \tserver k8s-master-1 &lt;kube-masterY-ip&gt;:6443 check     server k8s-master-2 &lt;kube-masterZ-ip&gt;:6443 check  Verify haproxy configuration &amp; restart HAproxy:  haproxy -f /etc/haproxy/haproxy.cfg -c   {     systemctl daemon-reload     sudo systemctl enable haproxy      sudo systemctl start haproxy     sudo systemctl status haproxy }  Install and configure Keepalived:  On both the nodes[master &amp; backup], run the following commands:  apt update &amp;&amp; apt install -y keepalived &amp;&amp; apt install -y libipset13  Keepalived Configurations:  On Master/Primary node:  vim /etc/keepalived/keepalived.conf   # Define the script used to check if haproxy is still working vrrp_script chk_haproxy {     script \"/usr/bin/killall -0 haproxy\"     interval 2     weight 2 }  # Configuration for Virtual Interface vrrp_instance LB_VIP {     interface ens6     state MASTER        # set to BACKUP on the peer machine     priority 301        # set to  300 on the peer machine     virtual_router_id 51      authentication {         auth_type user         auth_pass UGFzcwo=  # Password for accessing vrrpd. Same on all devices     }     unicast_src_ip &lt;lb-master-ip&gt; # IP address of master-lb     unicast_peer {         &lt;lb-backup-ip&gt;   # IP address of the backup-lb    }      # The virtual ip address shared between the two loadbalancers     virtual_ipaddress {         &lt;lb-vip&gt;    # vip      }     # Use the Defined Script to Check whether to initiate a fail over     track_script {         chk_haproxy     } }  On Backup/Secondary node:  vim /etc/keepalived/keepalived.conf   # Define the script used to check if haproxy is still working vrrp_script chk_haproxy {     script \"/usr/bin/killall -0 haproxy\"     interval 2     weight 2 }  # Configuration for Virtual Interface vrrp_instance LB_VIP {     interface ens6     state BACKUP        # set to BACKUP on the peer machine     priority 300        # set to  301 on the peer machine     virtual_router_id 51      authentication {         auth_type user         auth_pass UGFzcwo=  # Password for accessing vrrpd. Same on all devices     }     unicast_src_ip &lt;lb-backup-ip&gt; #IP address of backup-lb     unicast_peer {         &lt;lb-master-ip&gt;  #IP address of the master-lb    }      # The virtual ip address shared between the two loadbalancers     virtual_ipaddress {         &lt;lb-vip&gt;  #vip     }     # Use the Defined Script to Check whether to initiate a fail over.     track_script {         chk_haproxy     } }   Enable and restart keepalived service: {     systemctl enable --now keepalived     systemctl start keepalived     systemctl status keepalived }   üîó Reference Links:          Keepalived           Haproxy"
  },
  
  {
    "title": "Let's build a HA kubernetes cluster on baremetal servers",
    "url": "/posts/k8s-kubeadm-ha-cluster/",
    "categories": "kubernetes",
    "tags": "kubernetes",
    "date": "2025-11-03 14:25:00 +0530",
    "content": "HA Kubernetes cluster with containerd    üéûÔ∏è Watch Video  Prerequisites:    2 Ubuntu 24.04 LoadBalancer node‚Äôs   3 Ubuntu 24.04 Kubernetes master node‚Äôs   3 Ubuntu 24.04 Kubernetes worker node‚Äôs   Generate TLS certificates:  Prepare CFSSL on a Single Master Node (Certificate Authority Host): SSH into one of the master-node and run:  {   mkdir ~/certs &amp;&amp; cd ~/certs   wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64   wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64   chmod +x cfssl_linux-amd64 cfssljson_linux-amd64   mv cfssl_linux-amd64 /usr/local/bin/cfssl   mv cfssljson_linux-amd64 /usr/local/bin/cfssljson }   Create CA Config and CSR for Root Certificate Authority:  { cat &gt; ca-config.json &lt;&lt;EOF {     \"signing\": {         \"default\": {             \"expiry\": \"8766h\"         },         \"profiles\": {             \"kubernetes\": {                 \"expiry\": \"8766h\",                 \"usages\": [\"signing\",\"key encipherment\",\"server auth\",\"client auth\"]             }         }     } } EOF  cat &gt; ca-csr.json &lt;&lt;EOF {   \"CN\": \"Kubernetes\",   \"key\": {     \"algo\": \"rsa\",     \"size\": 2048   },   \"names\": [     {       \"C\": \"IN\",       \"L\": \"BLR\",       \"O\": \"Kubernetes\",       \"OU\": \"Cluster\",       \"ST\": \"KA\"     }   ] } EOF }   Generate CA cert and key:  cfssl gencert -initca ca-csr.json | cfssljson -bare ca  Create Server Certificate Signing Request (CSR) Including Full SANs:  cat &gt; kubernetes-csr.json &lt;&lt;EOF {   \"CN\": \"kubernetes\",   \"hosts\": [     \"127.0.0.1\",     \"kubernetes\",     \"kubernetes.default\",     \"kubernetes.default.svc\",     \"kubernetes.default.svc.cluster.local\",     \"&lt;Load balancer VIP&gt;\",         \"&lt;k8s-master-1-ip&gt;\",     \"&lt;k8s-master-2-ip&gt;\",     \"&lt;k8s-master-3-ip&gt;\",     \"&lt;load-01 IP&gt;\",          \"&lt;load-02 IP&gt;\"         ],   \"key\": {     \"algo\": \"rsa\",     \"size\": 2048   },   \"names\": [     {       \"C\": \"IN\",       \"L\": \"BLR\",       \"O\": \"Kubernetes\",       \"OU\": \"Cluster\",       \"ST\": \"KA\"     }   ] } EOF  Note: Remove comments while creating the csr.  Generate the Server Certificate Pair:  {   cfssl gencert \\   -ca=ca.pem \\   -ca-key=ca-key.pem \\   -config=ca-config.json \\   -profile=kubernetes kubernetes-csr.json | \\    cfssljson -bare kubernetes }   Generate Client/Admin Certificate for kubeadm and kubectl: cat &gt; admin-csr.json &lt;&lt;EOF {   \"CN\": \"admin\",   \"key\": {     \"algo\": \"rsa\",     \"size\": 2048   },   \"names\": [     {       \"C\": \"IN\",       \"L\": \"BLR\",       \"O\": \"system:masters\",       \"OU\": \"Cluster\",       \"ST\": \"KA\"     }   ] } EOF   Generate client cert: {   cfssl gencert \\   -ca=ca.pem \\   -ca-key=ca-key.pem \\   -config=ca-config.json \\   -profile=kubernetes \\   admin-csr.json | cfssljson -bare admin }  Copy admin.pem and admin-key.pem to your workstation/user where kubectl runs.  Distribute Certificates Securely: {   mkdir -p /etc/kubernetes/pki }   { declare -a NODES=(&lt;k8s-master-1-ip&gt; &lt;k8s-master-2-ip&gt; &lt;k8s-master-3-ip&gt;)  for node in ${NODES[@]}; do   scp ca.pem kubernetes.pem kubernetes-key.pem root@$node:/etc/kubernetes/pki/ done }   Setup kubernetes master &amp; worker nodes:  Run on all master &amp; worker nodes:  Load Kerenel Modules: { cat &lt;&lt; EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF  sudo modprobe overlay sudo modprobe br_netfilter }  Add Kernel Settings:  { cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF  sysctl --system }   Install container runtime: containerd  { wget https://github.com/containerd/containerd/releases/download/v2.1.4/containerd-2.1.4-linux-amd64.tar.gz tar Cxzvf /usr/local containerd-2.1.4-linux-amd64.tar.gz rm containerd-2.1.4-linux-amd64.tar.gz mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml }  sed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml   { mkdir -pv /usr/local/lib/systemd/system/ wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -O /usr/local/lib/systemd/system/containerd.service }  { systemctl daemon-reload systemctl enable --now containerd systemctl start containerd systemctl status containerd }   Disable swap permanently:  {   swapoff -a   sed -i '/ swap / s/^/#/' /etc/fstab }  Install runc: { wget https://github.com/opencontainers/runc/releases/download/v1.3.0/runc.amd64 install -m 755 runc.amd64 /usr/local/sbin/runc }   Installing CNI plugins:  { wget https://github.com/containernetworking/plugins/releases/download/v1.7.1/cni-plugins-linux-amd64-v1.7.1.tgz mkdir -p /opt/cni/bin tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.7.1.tgz }  Install cricctl: { VERSION=\"v1.34.0\" curl -L https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-${VERSION}-linux-amd64.tar.gz --output crictl-${VERSION}-linux-amd64.tar.gz sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/bin rm -f crictl-$VERSION-linux-amd64.tar.gz }   Set containerd as default runtime for crictl: { cat &lt;&lt; EOF | sudo tee /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 2 debug: false pull-image-on-create: false EOF }   Add Apt repository &amp; Install Kubernetes components: { apt-get install -y apt-transport-https ca-certificates curl gpg mkdir /etc/apt/keyrings curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg cat &lt;&lt; EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ / EOF }    { apt-get update apt-get install -y kubelet=1.34.0-1.1 kubeadm=1.34.0-1.1 kubectl=1.34.0-1.1 apt-mark hold kubelet kubeadm kubectl }    ETCD cluster creation[On all etcd nodes]:  Note: Copy ca.pem, kubernetes.pem, kubernetes-key.pem to other master nodes.  { declare -a NODES=(&lt;k8s-master-1-ip&gt; &lt;k8s-master-2-ip&gt; &lt;k8s-master-3-ip&gt;)  for node in ${NODES[@]}; do   scp ca.pem kubernetes.pem kubernetes-key.pem root@$node:  done }   {   mkdir -pv /etc/etcd /data/etcd   mv ca.pem kubernetes.pem kubernetes-key.pem /etc/etcd/   ll /etc/etcd/ }   Download etcd &amp; etcdctl binaries from Github:  {   ETCD_VER=v3.6.4   wget -q --show-progress \"https://github.com/etcd-io/etcd/releases/download/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz\"   tar zxf etcd-${ETCD_VER}-linux-amd64.tar.gz   mv etcd-${ETCD_VER}-linux-amd64/etcd* /usr/local/bin/   rm -rf etcd* }  Create systemd unit file for etcd service:  Set NODE_IP to the correct IP of the machine where you are running this {  NODE_IP=\"k8s-master-1\"  ETCD1_IP=\"k8s-master-1\" ETCD2_IP=\"k8s-master-2\" ETCD3_IP=\"k8s-master-3\"   cat &lt;&lt;EOF &gt;/etc/systemd/system/etcd.service [Unit] Description=etcd  [Service] Type=notify ExecStart=/usr/local/bin/etcd \\\\   --name ${NODE_IP} \\\\   --cert-file=/etc/etcd/kubernetes.pem \\\\   --key-file=/etc/etcd/kubernetes-key.pem \\\\   --peer-cert-file=/etc/etcd/kubernetes.pem \\\\   --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\   --trusted-ca-file=/etc/etcd/ca.pem \\\\   --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\   --peer-client-cert-auth \\\\   --client-cert-auth \\\\   --initial-advertise-peer-urls https://${NODE_IP}:2380 \\\\   --listen-peer-urls https://${NODE_IP}:2380 \\\\   --advertise-client-urls https://${NODE_IP}:2379 \\\\   --listen-client-urls https://${NODE_IP}:2379,https://127.0.0.1:2379 \\\\   --initial-cluster-token etcd-cluster-0 \\\\   --initial-cluster ${ETCD1_IP}=https://${ETCD1_IP}:2380,${ETCD2_IP}=https://${ETCD2_IP}:2380,${ETCD3_IP}=https://${ETCD3_IP}:2380 \\\\   --initial-cluster-state new \\\\   --data-dir=/data/etcd Restart=on-failure RestartSec=5  [Install] WantedBy=multi-user.target EOF  }   Enable and Start etcd service: {   systemctl daemon-reload   systemctl enable --now etcd   systemctl start etcd   systemctl status etcd }  Verify Etcd cluster status: { ETCDCTL_API=3 etcdctl member list \\   --endpoints=https://127.0.0.1:2379 \\   --cacert=/etc/etcd/kubernetes.pem \\   --cert=/etc/etcd/kubernetes.pem \\   --key=/etc/etcd/kubernetes-key.pem }  Example output: 25ef0cb30a2929f1, started, k8s-master-1, https://k8s-master-1:2380, https://k8s-master-1:2379, false 5818a496a39840ca, started, k8s-master-2, https://k8s-master-2:2380, https://k8s-master-2:2379, false c669cf505c64b0e8, started, k8s-master-3, https://k8s-master-3:2380, https://k8s-master-3:2379, false  Repeat the above spets on other master nodes by replacing the ip address with respect to node ip address.  Initializing Master Node 1:  Create configuration file : { HAPROXY_IP=\"haproxy-vip\"  MASTER01_IP=\"k8s-master-1\" MASTER02_IP=\"k8s-master-2\" MASTER03_IP=\"k8s-master-3\"  cat &lt;&lt;EOF &gt; config.yaml apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration kubernetesVersion: stable apiServer:   certSANs:   - ${HAPROXY_IP}   - ${MASTER01_IP}   - ${MASTER02_IP}   - ${MASTER03_IP}   - \"127.0.0.1\"   - \"kubernetes\"   - \"kubernetes.default\"   - \"kubernetes.default.svc\"   - \"kubernetes.default.svc.cluster.local\"   ExtraArgs:     apiserver-count: \"3\" controlPlaneEndpoint: \"${HAPROXY_IP}:6443\" etcd:   external:     endpoints:     - https://${MASTER01_IP}:2379     - https://${MASTER02_IP}:2379     - https://${MASTER03_IP}:2379     caFile: /etc/etcd/ca.pem     certFile: /etc/etcd/kubernetes.pem     keyFile: /etc/etcd/kubernetes-key.pem networking:   podSubnet: 10.244.0.0/16 apiServerExtraArgs:   apiserver-count: \"3\" EOF }  Initializing the master node:  kubeadm init --config=config.yaml --upload-certs   {       mkdir -p $HOME/.kube   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config   sudo chown $(id -u):$(id -g) $HOME/.kube/config }   Initializing Master Node 2 &amp; 3 :  Use the kubeadm join command printed after master01 init, with the ‚Äìcontrol-plane flag and the certificate key uploaded:  sudo kubeadm join 192.168.122.60:6443 --token &lt;token&gt; \\   --discovery-token-ca-cert-hash sha256:&lt;hash&gt; \\   --control-plane --certificate-key &lt;certificate-key&gt;   Adding Worker nodes:  Get the join command from master node and run it on worker nodes to join the cluster:  kubeadm token create --print-join-command  When you run kubeadm init you will get the join command as follows , you need to run this on all worker nodes.  kubeadm join 192.168.X.X:6443 --token 85iw5v.ymo1wqcs9mrqmnnf --discovery-token-ca-cert-hash sha256:4710a65a4f0a2be37c03249c83ca9df2377b4433c6564db4e61e9c07f5a213dd   Activate the flannel CNI plugin:  kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml   If you use custom podCIDR (not 10.244.0.0/16) you first need to download the above manifest and modify the network to match your one  üîó Reference Links:          Containerd           etcd           runc           cni plugins           cilium"
  }
  
]

